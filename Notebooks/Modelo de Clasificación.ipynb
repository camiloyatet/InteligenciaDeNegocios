{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org tabulate\n",
    "!pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org seaborn\n",
    "!pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org imblearn\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tabulate import tabulate\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para visualizar un conjunto de datos en 2D\n",
    "def plot_data(X, y, size=None):\n",
    "    y_unique = np.unique(y)\n",
    "    colors = pl.cm.rainbow(np.linspace(0.0, 1.0, y_unique.size))\n",
    "    for this_y, color in zip(y_unique, colors):\n",
    "        this_X = X[y == this_y]\n",
    "        pl.scatter(this_X[:, 0], this_X[:, 1],  c=color,\n",
    "                   alpha=0.5, edgecolor='k', s=size,\n",
    "                    label=\"Class %s\" % this_y)\n",
    "    pl.legend(loc=\"best\")\n",
    "    pl.title(\"Data\")\n",
    "    \n",
    "# Función para visualizar de la superficie de decisión de un clasificador\n",
    "def plot_decision_region2(X, pred_fun):      #Función para visualizar la superficie de decisión de nuestro algoritmo.\n",
    "    min_x = np.min(X[:, 0])\n",
    "    max_x = np.max(X[:, 0])\n",
    "    min_y = np.min(X[:, 1])\n",
    "    max_y = np.max(X[:, 1])\n",
    "    min_x = min_x - (max_x - min_x) * 0.05\n",
    "    max_x = max_x + (max_x - min_x) * 0.05\n",
    "    min_y = min_y - (max_y - min_y) * 0.05\n",
    "    max_y = max_y + (max_y - min_y) * 0.05\n",
    "    x_vals = np.linspace(min_x, max_x, 100)\n",
    "    y_vals = np.linspace(min_y, max_y, 100)\n",
    "    XX, YY = np.meshgrid(x_vals, y_vals)\n",
    "    grid_r, grid_c = XX.shape\n",
    "    ZZ = np.zeros((grid_r, grid_c))\n",
    "    for i in range(grid_r):\n",
    "        for j in range(grid_c):\n",
    "            ZZ[i, j] = pred_fun(XX[i, j], YY[i, j])\n",
    "    pl.contourf(XX, YY, ZZ, 100, cmap = pl.cm.coolwarm, vmin= 1, vmax=3)\n",
    "    pl.colorbar()\n",
    "    pl.xlabel(\"x\")\n",
    "    pl.ylabel(\"y\")\n",
    "    \n",
    "def gen_pred_fun(clf):\n",
    "    def pred_fun(x1, x2):\n",
    "        x = np.array([[x1, x2]])\n",
    "        return clf.predict(x)[0]\n",
    "    return pred_fun\n",
    "\n",
    "def list_cm(cm,classes):     #función para generar de una forma más visual la matriz de confusión\n",
    "    if len(cm)==2:\n",
    "      cm.astype(int)\n",
    "      row_0 =['','Valor','Verdadero']\n",
    "      row_1 =['-',classes[0],classes[1]]\n",
    "      row_2 =[classes[0],cm[0,0],cm[1,0]]\n",
    "      row_3 =[classes[1],cm[0,1],cm[1,1]]\n",
    "      table = zip(row_0,row_1, row_2, row_3)\n",
    "      headers = ['', '', 'Valor', 'Predicho']  \n",
    "      return print(tabulate(table, headers=headers, floatfmt=\".0f\"))\n",
    "    else:\n",
    "      cm.astype(int)\n",
    "      row_0 =['','Valor','Verdadero','']\n",
    "      row_1 =['-',np.int(classes[0]),classes[1],classes[2]]\n",
    "      row_2 =[classes[0],cm[0,0],cm[1,0],cm[2,0]]\n",
    "      row_3 =[classes[1],cm[0,1],cm[1,1],cm[2,1]]\n",
    "      row_4 =[classes[2],cm[0,2],cm[1,2],cm[2,2]]\n",
    "      table = zip(row_0,row_1, row_2, row_3, row_4)\n",
    "      headers = ['', '', 'Valor', 'Predicho', '']  \n",
    "      return print(tabulate(table, headers=headers, floatfmt=\".0f\")) \n",
    "\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Matriz de confusión',\n",
    "                          cmap=pl.cm.Blues):\n",
    "    print(cm) # Confusion matrix\n",
    "\n",
    "    pl.imshow(cm, interpolation='nearest', cmap=cmap) # Pintamos la matriz como una imagen\n",
    "    pl.title(title)\n",
    "    pl.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    pl.xticks(tick_marks, classes, rotation=45) # Nombre de las clases en X\n",
    "    pl.yticks(tick_marks, classes) # Nombre de las clases en Y\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        pl.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\") \n",
    "        # Anotamos cada sección de la imagen con su valor correspondiente en la matriz\n",
    "\n",
    "    pl.tight_layout()\n",
    "    pl.ylabel('Valor de verdad')\n",
    "    pl.xlabel('Valor predicho')\n",
    "\n",
    "    \n",
    "    %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from sklearn.datasets import make_circles\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "\n",
    "# Función para visualizar un conjunto de datos en 2D\n",
    "def plot_data(X, y):\n",
    "    y_unique = np.unique(y)\n",
    "    colors = pl.cm.rainbow(np.linspace(0.0, 1.0, y_unique.size))\n",
    "    for this_y, color in zip(y_unique, colors):\n",
    "        this_X = X[y == this_y]\n",
    "        pl.scatter(this_X[:, 0], this_X[:, 1],  c=color,\n",
    "                    alpha=0.5, edgecolor='k',\n",
    "                    label=\"Class %s\" % this_y)\n",
    "    pl.legend(loc=\"best\")\n",
    "    pl.title(\"Data\")\n",
    "    \n",
    "# Función para visualizar de la superficie de decisión de un clasificador\n",
    "def plot_decision_region(X, pred_fun):\n",
    "    min_x = np.min(X[:, 0])\n",
    "    max_x = np.max(X[:, 0])\n",
    "    min_y = np.min(X[:, 1])\n",
    "    max_y = np.max(X[:, 1])\n",
    "    min_x = min_x - (max_x - min_x) * 0.05\n",
    "    max_x = max_x + (max_x - min_x) * 0.05\n",
    "    min_y = min_y - (max_y - min_y) * 0.05\n",
    "    max_y = max_y + (max_y - min_y) * 0.05\n",
    "    x_vals = np.linspace(min_x, max_x, 100)\n",
    "    y_vals = np.linspace(min_y, max_y, 100)\n",
    "    XX, YY = np.meshgrid(x_vals, y_vals)\n",
    "    grid_r, grid_c = XX.shape\n",
    "    ZZ = np.zeros((grid_r, grid_c))\n",
    "    for i in range(grid_r):\n",
    "        for j in range(grid_c):\n",
    "            ZZ[i, j] = pred_fun(XX[i, j], YY[i, j])\n",
    "    pl.contourf(XX, YY, ZZ, 100, cmap = pl.cm.coolwarm, vmin= -1, vmax=2)\n",
    "    pl.colorbar()\n",
    "    pl.xlabel(\"x\")\n",
    "    pl.ylabel(\"y\")\n",
    "    \n",
    "class MidpointNormalize(Normalize):\n",
    "\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y))\n",
    "    \n",
    "def gen_pred_fun(clf):\n",
    "    def pred_fun(x1, x2):\n",
    "        x = np.array([[x1, x2]])\n",
    "        return clf.predict(x)[0]\n",
    "    return pred_fun\n",
    "\n",
    "def list_cm(cm,classes):     #función para generar de una forma más visual la matriz de confusión\n",
    "    if len(cm)==2:\n",
    "      cm.astype(int)\n",
    "      row_0 =['','Valor','Verdadero']\n",
    "      row_1 =['-',classes[0],classes[1]]\n",
    "      row_2 =[classes[0],cm[0,0],cm[1,0]]\n",
    "      row_3 =[classes[1],cm[0,1],cm[1,1]]\n",
    "      table = zip(row_0,row_1, row_2, row_3)\n",
    "      headers = ['', '', 'Valor', 'Predicho']  \n",
    "      return print(tabulate(table, headers=headers, floatfmt=\".0f\"))\n",
    "    else:\n",
    "      cm.astype(int)\n",
    "      row_0 =['','Valor','Verdadero','']\n",
    "      row_1 =['-',np.int(classes[0]),classes[1],classes[2]]\n",
    "      row_2 =[classes[0],cm[0,0],cm[1,0],cm[2,0]]\n",
    "      row_3 =[classes[1],cm[0,1],cm[1,1],cm[2,1]]\n",
    "      row_4 =[classes[2],cm[0,2],cm[1,2],cm[2,2]]\n",
    "      table = zip(row_0,row_1, row_2, row_3, row_4)\n",
    "      headers = ['', '', 'Valor', 'Predicho', '']\n",
    "      return print(tabulate(table, headers=headers, floatfmt=\".0f\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "df = pd.read_csv(\"BD/BaseFinal.csv\", encoding = 'latin1', sep = \";\", decimal = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categoricas=['genero', 'clasedecontrato', 'regional', 'departamento', 'ciudad', 'tipodevinculo', 'tipodeposicion', \n",
    "             'posicion', 'horassemanalespromedio', 'motivodeaprobaciongral', 'turnoposicion', 'familiasdecargo', \n",
    "             'divisiondepersonal', 'subdivisiondepersonal', 'areaorganizativa', 'nopersonaljefe','posicioncargodejefe',\n",
    "             'Categoria', 'Segmento_poblacional','estado_civil', 'nivel_academico', 'segmento_grupo_familiar', \n",
    "             'DepartamentoPersona', 'MunicipioPersona','TipoPoblado',  'EstratoPersona', 'filial_famisanar', \n",
    "             'filial_ips_colsubsidio', 'filial_ips_cafam', 'filial_otra_ips', 'filial_pac_famisanar', 'filial_bancompartir', \n",
    "             'marca_aventurero','marca_tu_club','filial_suramericana', 'filial_pac_suramericana', 'filial_proteccion', \n",
    "             'filial_pension_obligatoria','filial_pension_voluntaria', 'filial_cesantias', 'CS_As_Hipo', 'CS_As_Libr', \n",
    "             'CS_As_NoLibranza', 'CS_As_Titular', 'CS_CO_CsConv', 'MS_Drog', 'MS_Sup', 'Ryt_AgViaje', 'Ryt_ClBellavista', \n",
    "             'Ryt_ClCll195','Ryt_ClColina', 'Ryt_ClCubo', 'Ryt_HoAlcaravan', 'Ryt_HoAthan', 'Ryt_HoPaipa', 'Ryt_HoPeÃ±alisa', \n",
    "             'Ryt_Piscilago', 'Ryt_PrDeport', 'Ryt_PrRecRyt', 'Salud_NoPos', 'Salud_Pos', 'Riesgo']\n",
    "\n",
    "for var in categoricas:\n",
    "    df[var] = df[var].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=df.drop(['nodocumento','nombre','fechadeingreso','ubicacion','FechaNacimiento','unidaddirectiva','unidadestrategica','unidadorganizativa','Poblado','PrimeraFalta'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(lambda x: x.isnull().sum(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricas=['genero', 'clasedecontrato', 'regional', 'departamento', 'ciudad', 'tipodevinculo', 'tipodeposicion', \n",
    "             'posicion', 'horassemanalespromedio', 'motivodeaprobaciongral', 'turnoposicion', 'familiasdecargo', \n",
    "             'divisiondepersonal', 'subdivisiondepersonal', 'areaorganizativa', 'nopersonaljefe','posicioncargodejefe',\n",
    "             'Categoria', 'Segmento_poblacional','estado_civil', 'nivel_academico', 'segmento_grupo_familiar', \n",
    "             'DepartamentoPersona', 'MunicipioPersona','TipoPoblado',  'EstratoPersona', 'filial_famisanar', \n",
    "             'filial_ips_colsubsidio', 'filial_ips_cafam', 'filial_otra_ips', 'filial_pac_famisanar', 'filial_bancompartir', \n",
    "             'marca_aventurero','marca_tu_club','filial_suramericana', 'filial_pac_suramericana', 'filial_proteccion', \n",
    "             'filial_pension_obligatoria','filial_pension_voluntaria', 'filial_cesantias', 'CS_As_Hipo', 'CS_As_Libr', \n",
    "             'CS_As_NoLibranza', 'CS_As_Titular', 'CS_CO_CsConv', 'MS_Drog', 'MS_Sup', 'Ryt_AgViaje', 'Ryt_ClBellavista', \n",
    "             'Ryt_ClCll195','Ryt_ClColina', 'Ryt_ClCubo', 'Ryt_HoAlcaravan', 'Ryt_HoAthan', 'Ryt_HoPaipa', 'Ryt_HoPeÃ±alisa', \n",
    "             'Ryt_Piscilago', 'Ryt_PrDeport', 'Ryt_PrRecRyt', 'Salud_NoPos', 'Salud_Pos']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df1 = pd.get_dummies(df, columns=categoricas, prefix=['genero', 'clasedecontrato', 'regional', 'departamento', 'ciudad', 'tipodevinculo', 'tipodeposicion', \n",
    "             'posicion', 'horassemanalespromedio', 'motivodeaprobaciongral', 'turnoposicion', 'familiasdecargo', \n",
    "             'divisiondepersonal', 'subdivisiondepersonal', 'areaorganizativa', 'nopersonaljefe','posicioncargodejefe',\n",
    "             'Categoria', 'Segmento_poblacional','estado_civil', 'nivel_academico', 'segmento_grupo_familiar', \n",
    "             'DepartamentoPersona', 'MunicipioPersona','TipoPoblado',  'EstratoPersona', 'filial_famisanar', \n",
    "             'filial_ips_colsubsidio', 'filial_ips_cafam', 'filial_otra_ips', 'filial_pac_famisanar', 'filial_bancompartir', \n",
    "             'marca_aventurero','marca_tu_club','filial_suramericana', 'filial_pac_suramericana', 'filial_proteccion', \n",
    "             'filial_pension_obligatoria','filial_pension_voluntaria', 'filial_cesantias', 'CS_As_Hipo', 'CS_As_Libr', \n",
    "             'CS_As_NoLibranza', 'CS_As_Titular', 'CS_CO_CsConv', 'MS_Drog', 'MS_Sup', 'Ryt_AgViaje', 'Ryt_ClBellavista', \n",
    "             'Ryt_ClCll195','Ryt_ClColina', 'Ryt_ClCubo', 'Ryt_HoAlcaravan', 'Ryt_HoAthan', 'Ryt_HoPaipa', 'Ryt_HoPeÃ±alisa', \n",
    "             'Ryt_Piscilago', 'Ryt_PrDeport', 'Ryt_PrRecRyt', 'Salud_NoPos', 'Salud_Pos'])\n",
    "\n",
    "X = df1.drop(['Riesgo'], axis = 1)\n",
    "y = df[['Riesgo']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=31415)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm = SMOTE(ratio=0.5, random_state=31415)\n",
    "X_res, y_res = sm.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimación de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.svm import LinearSVC \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'GradientBoosting': GradientBoostingClassifier(random_state=31415, n_estimators=1250)\n",
    "          ,'RandomForest' : RandomForestClassifier(n_jobs=-1)\n",
    "          ,'DesicionTree' : DecisionTreeClassifier(max_depth=5)\n",
    "}\n",
    "\n",
    "for model in models.keys():\n",
    "    models[model].fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame(index=X_test.index, columns=models.keys())\n",
    "for model in models.keys():\n",
    "    y_pred[model] = models[model].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df['Riesgo'].unique()\n",
    "print(classes)\n",
    "for model in models.keys():\n",
    "    cnf_matrix_1 = confusion_matrix(y_test, y_pred[model])\n",
    "    print(model)\n",
    "    list_cm(cnf_matrix_1, classes = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "for model in models.keys():\n",
    "    print(model)\n",
    "    print('Accurracy: {}'.format(models[model].score(X_test, y_test)))\n",
    "    print('Error de clasificación: {}'.format(1 - models[model].score(X_test, y_test)))\n",
    "    print('Precisión macro: {}'.format(precision_score(y_test, y_pred[model], average='macro')))\n",
    "    print('Recall macro: {}'.format(recall_score(y_test, y_pred[model], average='macro')))\n",
    "    print('F1 macro: {}'.format(f1_score(y_test,y_pred[model], average='macro')))\n",
    "    print('AUC: {}'.format(roc_auc_score(y_test, models[model].predict_proba(X_test)[:, 1]), average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "FinalMod=models['GradientBoosting']\n",
    "FinalMod1=models['DesicionTree']\n",
    "\n",
    "n_cv=5\n",
    "scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro', 'roc_auc']\n",
    "\n",
    "score=cross_validate(FinalMod, X_res, y_res, cv=n_cv, scoring=scoring)\n",
    "print(\"train: \\n\", score1, \"\\n\")\n",
    "\n",
    "score=cross_validate(FinalMod, X_test, y_test, cv=n_cv, scoring=scoring)\n",
    "print(\"test: \\n\", score1)\n",
    "\n",
    "score1=cross_validate(FinalMod1, X_res, y_res, cv=n_cv, scoring=scoring)\n",
    "print(\"train: \\n\", score1, \"\\n\")\n",
    "\n",
    "score1=cross_validate(FinalMod1, X_test, y_test, cv=n_cv, scoring=scoring)\n",
    "print(\"test: \\n\", score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Proba = FinalMod.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Fin = pd.concat([df.reset_index(drop=True), pd.DataFrame(Proba)], axis=1)\n",
    "Data_Fin.to_csv('Resultados/Riesgo_Calif.csv', sep='|', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Almacenamiento de la importancia de los predictores.\n",
    "#importances = FinalMod.feature_importances_\n",
    "#indices = np.argsort(importances)\n",
    "\n",
    "# Gráfico de importancia de los predictores.\n",
    "#plt.figure(1)\n",
    "#plt.title('Importancia de los Predictores')\n",
    "#plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "#plt.yticks(range(len(indices)), [X[i] for i in indices])\n",
    "#plt.xlabel('Importancia Relativa')\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(FinalMod, 'Resultados/GradienteBoosting.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
